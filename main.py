import os
import pickle
import json
import lancedb
import asyncio
from fastapi import FastAPI, Request, HTTPException, Depends
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
from typing import List, Optional
from dotenv import load_dotenv

# Import the core logic components
from sentence_transformers import SentenceTransformer, CrossEncoder
from rank_bm25 import BM25Okapi
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMExtractionStrategy, LLMConfig

# --- Configuration & Models ---
load_dotenv()
API_TOKEN = "3e0af1293189f8f9129d33fb7d568a40c997067afa84f791cbd17e9404b5a35c" # From details.txt
TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY")

# --- Pydantic Models for API ---
class QueryRequest(BaseModel):
    documents: str
    questions: List[str]

class QueryResponse(BaseModel):
    answers: List[str]

class AdjudicationResponse(BaseModel):
    decision: str
    amount: Optional[str] = "Not Applicable"
    justification: str

# --- The Adjudicator Engine (Accuracy-Focused Version) ---
class Adjudicator:
    def __init__(self, db_path, bm25_path):
        print("ðŸš€ Initializing Adjudicator Engine...")
        # Load Cognitive Core
        db = lancedb.connect(db_path)
        self.table = db.open_table("policies")
        with open(bm25_path, "rb") as f:
            self.bm25 = pickle.load(f)
        self.corpus_texts = self.table.to_pandas()['text'].tolist()
        
        # Initialize Models
        self.dense_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
        self.llm_config = LLMConfig(provider="together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1", api_token=TOGETHER_API_KEY)
        print("âœ… Engine Ready!")

    def _retrieve_and_rerank(self, query: str) -> List[str]:
        # --- This is our high-accuracy retrieval logic ---
        query_vector = self.dense_model.encode(query)
        vector_results = self.table.search(query_vector).limit(20).to_pandas()
        tokenized_query = query.lower().split(" ")
        bm25_scores = self.bm25.get_scores(tokenized_query)
        top_bm25_indices = sorted(range(len(bm25_scores)), key=lambda i: bm25_scores[i], reverse=True)[:20]
        combined_indices = set(vector_results.index) | set(top_bm25_indices)
        sentence_pairs = [[query, self.corpus_texts[i]] for i in combined_indices]
        scores = self.cross_encoder.predict(sentence_pairs)
        reranked_results = sorted(zip(scores, combined_indices), key=lambda x: x[0], reverse=True)
        return [self.corpus_texts[idx] for score, idx in reranked_results[:15]] # Use top 15 for context

    async def adjudicate_single_question(self, query: str) -> str:
        clauses = self._retrieve_and_rerank(query)
        if not clauses:
            return "Could not retrieve relevant information to answer the question."

        context = "\n\n---\n\n".join(clauses)
        prompt = f"""You are an expert insurance claim adjudicator. Your task is to make a final decision based ONLY on the provided Policy Clauses and return a single JSON object.

**User's Claim:**
"{query}"

**Policy Clauses to Analyze:**
{context}

Based on the clauses, determine the 'decision', 'amount', and 'justification'. The justification must be a concise explanation. Respond with nothing but the required JSON object."""

        strategy = LLMExtractionStrategy(
            llm_config=self.llm_config,
            schema=AdjudicationResponse.model_json_schema(),
            instruction=prompt,
            extraction_type="schema"
        )
        
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url="raw://placeholder", config=CrawlerRunConfig(extraction_strategy=strategy))
        
        try:
            raw_output = json.loads(result.extracted_content)
            if isinstance(raw_output, list) and raw_output:
                return raw_output[0].get('justification', "Error: Justification not found.")
            elif isinstance(raw_output, dict):
                return raw_output.get('justification', "Error: Justification not found.")
        except Exception:
            return "Error: Failed to parse a valid answer from the model."
        return "Could not generate a conclusive answer."

# --- FastAPI Application ---
app = FastAPI()
bearer_scheme = HTTPBearer()

# Load the adjudicator engine on startup
DB_PATH = "./policies.lancedb"
BM25_PATH = "./bm25_index.pkl"
adjudicator = Adjudicator(db_path=DB_PATH, bm25_path=BM25_PATH)

def verify_token(credentials: HTTPAuthorizationCredentials = Depends(bearer_scheme)):
    if credentials.scheme != "Bearer" or credentials.credentials != API_TOKEN:
        raise HTTPException(status_code=401, detail="Invalid or missing API token")

@app.post("/hackrx/run", response_model=QueryResponse, dependencies=[Depends(verify_token)])
async def process_queries(request: QueryRequest):
    # NOTE: For this accuracy-first submission, we are ignoring the request.documents URL
    # and using the Cognitive Core we already built from our 6 PDFs.
    
    answers = []
    for question in request.questions:
        answer = await adjudicator.adjudicate_single_question(question)
        answers.append(answer)
        
    return QueryResponse(answers=answers)